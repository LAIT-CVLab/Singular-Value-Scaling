{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "cfg_specs = {\n",
    "    'auto':      dict(ref_gpus=-1, kimg=25000,  mb=-1, mbstd=-1, fmaps=-1,  lrate=-1,     gamma=-1,   ema=-1,  ramp=0.05, map=2), # Populated dynamically based on resolution and GPU count.\n",
    "    'stylegan2': dict(ref_gpus=8,  kimg=25000,  mb=32, mbstd=4,  fmaps=1,   lrate=0.002,  gamma=10,   ema=10,  ramp=None, map=8), # Uses mixed-precision, unlike the original StyleGAN2.\n",
    "    'paper256':  dict(ref_gpus=8,  kimg=25000,  mb=64, mbstd=8,  fmaps=0.5, lrate=0.0025, gamma=1,    ema=20,  ramp=None, map=8),\n",
    "    'paper512':  dict(ref_gpus=8,  kimg=25000,  mb=64, mbstd=8,  fmaps=1,   lrate=0.0025, gamma=0.5,  ema=20,  ramp=None, map=8),\n",
    "    'paper1024': dict(ref_gpus=8,  kimg=25000,  mb=32, mbstd=4,  fmaps=1,   lrate=0.002,  gamma=2,    ema=10,  ramp=None, map=8),\n",
    "    'cifar':     dict(ref_gpus=2,  kimg=100000, mb=64, mbstd=32, fmaps=1,   lrate=0.0025, gamma=0.01, ema=500, ramp=0.05, map=2),\n",
    "}\n",
    "\n",
    "cfg = 'stylegan2' # ['stylegan2', 'paper256]\n",
    "img_resolution = 256 # [256, 1024]\n",
    "ckpt_path = '/path/to/unofficial/weight/path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_channels = 3\n",
    "\n",
    "args = dnnlib.EasyDict()\n",
    "assert cfg in cfg_specs\n",
    "spec = dnnlib.EasyDict(cfg_specs[cfg])\n",
    "\n",
    "args.G_kwargs = dnnlib.EasyDict(class_name='training.networks.Generator', z_dim=512, w_dim=512, mapping_kwargs=dnnlib.EasyDict(), synthesis_kwargs=dnnlib.EasyDict())\n",
    "args.D_kwargs = dnnlib.EasyDict(class_name='training.networks.Discriminator', block_kwargs=dnnlib.EasyDict(), mapping_kwargs=dnnlib.EasyDict(), epilogue_kwargs=dnnlib.EasyDict())\n",
    "args.G_kwargs.synthesis_kwargs.channel_base = args.D_kwargs.channel_base = int(spec.fmaps * 32768)\n",
    "args.G_kwargs.synthesis_kwargs.channel_max = args.D_kwargs.channel_max = 512\n",
    "args.G_kwargs.mapping_kwargs.num_layers = spec.map\n",
    "args.G_kwargs.synthesis_kwargs.num_fp16_res = args.D_kwargs.num_fp16_res = 4 # enable mixed-precision training\n",
    "args.G_kwargs.synthesis_kwargs.conv_clamp = args.D_kwargs.conv_clamp = 256 # clamp activations to avoid float16 overflow\n",
    "args.D_kwargs.epilogue_kwargs.mbstd_group_size = spec.mbstd\n",
    "args.G_kwargs[\"class_name\"] = \"training.networks.Generator\"\n",
    "\n",
    "# Pruned model\n",
    "\n",
    "# args.G_kwargs[\"class_name\"] = \"training.networks_stylekd.Generator\"\n",
    "# args.G_kwargs[\"prune_ratio\"] = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "common_kwargs = dict(c_dim=0, img_resolution=img_resolution, img_channels=img_channels)\n",
    "G_ema = dnnlib.util.construct_class_by_name(**args.G_kwargs, **common_kwargs).eval().requires_grad_(False).to(device)\n",
    "D = dnnlib.util.construct_class_by_name(**args.D_kwargs, **common_kwargs).eval().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')['g_ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mapping(ckpt_from, G_ema):\n",
    "    mapping_dict = G_ema.mapping.state_dict()\n",
    "    len_map = (len(mapping_dict.keys())-1)//2\n",
    "    \n",
    "    for idx in range(len_map):\n",
    "        from_weight_key = f\"style.{idx+1}.weight\"\n",
    "        from_bias_key = f\"style.{idx+1}.bias\"\n",
    "        \n",
    "        to_weight_key = f\"mapping.fc{idx}.weight\"\n",
    "        to_bias_key = f\"mapping.fc{idx}.bias\"\n",
    "        \n",
    "        ckpt_from[to_weight_key] = ckpt_from[from_weight_key]\n",
    "        ckpt_from[to_bias_key] = ckpt_from[from_bias_key]\n",
    "        \n",
    "        del ckpt_from[from_weight_key]    \n",
    "        del ckpt_from[from_bias_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_torgb(ckpt_from, G_ema):\n",
    "    channels_res = [int(res) for res in 4*2**np.arange(np.log2(G_ema.img_resolution)-1)]\n",
    "    \n",
    "    res4 = channels_res.pop(0)\n",
    "    from_weight_key = f\"to_rgb1.conv.weight\"\n",
    "    from_bias_key = f\"to_rgb1.bias\"\n",
    "    from_affine_weight_key = f\"to_rgb1.conv.modulation.weight\"\n",
    "    from_affine_bias_key = f\"to_rgb1.conv.modulation.bias\"\n",
    "    \n",
    "    to_weight_key = f\"synthesis.b4.torgb.weight\"\n",
    "    to_bias_key = f\"synthesis.b4.torgb.bias\"\n",
    "    to_affine_weight_key = f\"synthesis.b4.torgb.affine.weight\"\n",
    "    to_affine_bias_key = f\"synthesis.b4.torgb.affine.bias\"\n",
    "    \n",
    "    ckpt_from[to_weight_key] = ckpt_from[from_weight_key][0]\n",
    "    ckpt_from[to_bias_key] = ckpt_from[from_bias_key].squeeze()\n",
    "    ckpt_from[to_affine_weight_key] = ckpt_from[from_affine_weight_key]\n",
    "    ckpt_from[to_affine_bias_key] = ckpt_from[from_affine_bias_key]\n",
    "    del ckpt_from[from_weight_key]    \n",
    "    del ckpt_from[from_bias_key]\n",
    "    del ckpt_from[from_affine_weight_key]    \n",
    "    del ckpt_from[from_affine_bias_key]\n",
    "    \n",
    "    \n",
    "    for idx, res in enumerate(channels_res):\n",
    "        from_weight_key = f\"to_rgbs.{idx}.conv.weight\"\n",
    "        from_bias_key = f\"to_rgbs.{idx}.bias\"\n",
    "        from_affine_weight_key = f\"to_rgbs.{idx}.conv.modulation.weight\"\n",
    "        from_affine_bias_key = f\"to_rgbs.{idx}.conv.modulation.bias\"\n",
    "        \n",
    "        to_weight_key = f\"synthesis.b{res}.torgb.weight\"\n",
    "        to_bias_key = f\"synthesis.b{res}.torgb.bias\"\n",
    "        to_affine_weight_key = f\"synthesis.b{res}.torgb.affine.weight\"\n",
    "        to_affine_bias_key = f\"synthesis.b{res}.torgb.affine.bias\"\n",
    "        \n",
    "        ckpt_from[to_weight_key] = ckpt_from[from_weight_key][0]\n",
    "        ckpt_from[to_bias_key] = ckpt_from[from_bias_key].squeeze()\n",
    "        ckpt_from[to_affine_weight_key] = ckpt_from[from_affine_weight_key]\n",
    "        ckpt_from[to_affine_bias_key] = ckpt_from[from_affine_bias_key]\n",
    "        \n",
    "        del ckpt_from[from_weight_key]    \n",
    "        del ckpt_from[from_bias_key]\n",
    "        del ckpt_from[from_affine_weight_key]    \n",
    "        del ckpt_from[from_affine_bias_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_const(ckpt_from, G_ema):\n",
    "    from_const_key = f\"input.input\"\n",
    "    to_const_key = f\"synthesis.b4.const\"\n",
    "    ckpt_from[to_const_key] = ckpt_from[from_const_key][0]\n",
    "    \n",
    "    del ckpt_from[from_const_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_convs(ckpt_from, G_ema):\n",
    "    channels_res = [int(res) for res in 4*2**np.arange(np.log2(G_ema.img_resolution)-1)]\n",
    "    \n",
    "    res4 = channels_res.pop(0)\n",
    "    from_weight_key = f\"conv1.conv.weight\"\n",
    "    from_bias_key = f\"conv1.activate.bias\"\n",
    "    from_affine_weight_key = f\"conv1.conv.modulation.weight\"\n",
    "    from_affine_bias_key = f\"conv1.conv.modulation.bias\"\n",
    "    from_noise_weight_key = f\"conv1.noise.weight\"\n",
    "    \n",
    "    to_weight_key = f\"synthesis.b4.conv1.weight\"\n",
    "    to_bias_key = f\"synthesis.b4.conv1.bias\"\n",
    "    to_affine_weight_key = f\"synthesis.b4.conv1.affine.weight\"\n",
    "    to_affine_bias_key = f\"synthesis.b4.conv1.affine.bias\"\n",
    "    to_noise_weight_key = f\"synthesis.b4.conv1.noise_strength\"\n",
    "    \n",
    "    ckpt_from[to_weight_key] = ckpt_from[from_weight_key][0]\n",
    "    ckpt_from[to_bias_key] = ckpt_from[from_bias_key]\n",
    "    ckpt_from[to_affine_weight_key] = ckpt_from[from_affine_weight_key]\n",
    "    ckpt_from[to_affine_bias_key] = ckpt_from[from_affine_bias_key]\n",
    "    ckpt_from[to_noise_weight_key] = ckpt_from[from_noise_weight_key].squeeze()\n",
    "    \n",
    "    del ckpt_from[from_weight_key]    \n",
    "    del ckpt_from[from_bias_key]\n",
    "    del ckpt_from[from_affine_weight_key]\n",
    "    del ckpt_from[from_affine_bias_key]\n",
    "    del ckpt_from[from_noise_weight_key]\n",
    "    \n",
    "    \n",
    "    for idx, res in enumerate(channels_res):\n",
    "        for i, _idx in enumerate([2*idx, 2*idx+1]):\n",
    "            from_weight_key = f\"convs.{_idx}.conv.weight\"\n",
    "            from_bias_key = f\"convs.{_idx}.activate.bias\"\n",
    "            from_affine_weight_key = f\"convs.{_idx}.conv.modulation.weight\"\n",
    "            from_affine_bias_key = f\"convs.{_idx}.conv.modulation.bias\"\n",
    "            from_noise_weight_key = f\"convs.{_idx}.noise.weight\"\n",
    "            \n",
    "            to_weight_key = f\"synthesis.b{res}.conv{i}.weight\"\n",
    "            to_bias_key = f\"synthesis.b{res}.conv{i}.bias\"\n",
    "            to_affine_weight_key = f\"synthesis.b{res}.conv{i}.affine.weight\"\n",
    "            to_affine_bias_key = f\"synthesis.b{res}.conv{i}.affine.bias\"\n",
    "            to_noise_weight_key = f\"synthesis.b{res}.conv{i}.noise_strength\"\n",
    "            \n",
    "            ckpt_from[to_weight_key] = ckpt_from[from_weight_key][0]\n",
    "            ckpt_from[to_bias_key] = ckpt_from[from_bias_key]\n",
    "            ckpt_from[to_affine_weight_key] = ckpt_from[from_affine_weight_key]\n",
    "            ckpt_from[to_affine_bias_key] = ckpt_from[from_affine_bias_key]\n",
    "            ckpt_from[to_noise_weight_key] = ckpt_from[from_noise_weight_key].squeeze()\n",
    "            \n",
    "            del ckpt_from[from_weight_key]    \n",
    "            del ckpt_from[from_bias_key]\n",
    "            del ckpt_from[from_affine_weight_key]\n",
    "            del ckpt_from[from_affine_bias_key]\n",
    "            del ckpt_from[from_noise_weight_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_mapping(ckpt, G_ema)\n",
    "convert_torgb(ckpt, G_ema)\n",
    "convert_const(ckpt, G_ema)\n",
    "convert_convs(ckpt, G_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['synthesis.b4.resample_filter', 'synthesis.b4.conv1.resample_filter', 'synthesis.b4.conv1.noise_const', 'synthesis.b8.resample_filter', 'synthesis.b8.conv0.resample_filter', 'synthesis.b8.conv0.noise_const', 'synthesis.b8.conv1.resample_filter', 'synthesis.b8.conv1.noise_const', 'synthesis.b16.resample_filter', 'synthesis.b16.conv0.resample_filter', 'synthesis.b16.conv0.noise_const', 'synthesis.b16.conv1.resample_filter', 'synthesis.b16.conv1.noise_const', 'synthesis.b32.resample_filter', 'synthesis.b32.conv0.resample_filter', 'synthesis.b32.conv0.noise_const', 'synthesis.b32.conv1.resample_filter', 'synthesis.b32.conv1.noise_const', 'synthesis.b64.resample_filter', 'synthesis.b64.conv0.resample_filter', 'synthesis.b64.conv0.noise_const', 'synthesis.b64.conv1.resample_filter', 'synthesis.b64.conv1.noise_const', 'synthesis.b128.resample_filter', 'synthesis.b128.conv0.resample_filter', 'synthesis.b128.conv0.noise_const', 'synthesis.b128.conv1.resample_filter', 'synthesis.b128.conv1.noise_const', 'synthesis.b256.resample_filter', 'synthesis.b256.conv0.resample_filter', 'synthesis.b256.conv0.noise_const', 'synthesis.b256.conv1.resample_filter', 'synthesis.b256.conv1.noise_const', 'mapping.w_avg'], unexpected_keys=['convs.0.conv.blur.kernel', 'convs.2.conv.blur.kernel', 'convs.4.conv.blur.kernel', 'convs.6.conv.blur.kernel', 'convs.8.conv.blur.kernel', 'convs.10.conv.blur.kernel', 'to_rgbs.0.upsample.kernel', 'to_rgbs.1.upsample.kernel', 'to_rgbs.2.upsample.kernel', 'to_rgbs.3.upsample.kernel', 'to_rgbs.4.upsample.kernel', 'to_rgbs.5.upsample.kernel', 'noises.noise_0', 'noises.noise_1', 'noises.noise_2', 'noises.noise_3', 'noises.noise_4', 'noises.noise_5', 'noises.noise_6', 'noises.noise_7', 'noises.noise_8', 'noises.noise_9', 'noises.noise_10', 'noises.noise_11', 'noises.noise_12'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_ema.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_pickle = {}\n",
    "for name, module in [('G', G_ema), ('D', D), ('G_ema', G_ema)]:\n",
    "    module = copy.deepcopy(module).eval().requires_grad_(False).cpu()\n",
    "    converted_pickle[name] = module\n",
    "with open('converted_model.pkl', 'wb') as f:\n",
    "    pickle.dump(converted_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dnnlib.util.open_url('converted_model.pkl') as f:\n",
    "    loaded_G = legacy.load_network_pkl(f)['G_ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
